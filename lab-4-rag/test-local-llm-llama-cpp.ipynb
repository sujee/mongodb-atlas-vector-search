{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Local Model\n",
    "\n",
    "This notebook shows how to run a local model on your computer\n",
    "\n",
    "We will be using llama CPP\n",
    "\n",
    "References:\n",
    "- https://github.com/abetlen/llama-cpp-python\n",
    "- https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/llm/llama_2_llama_cpp.ipynb\n",
    "- https://colab.research.google.com/drive/1UoPcoiA5EOBghxWKWduQhChliMHxla7U?usp=sharing#scrollTo=V1DwVkHCNDgT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1:  Installing LLama-CPP-Python\n",
    "\n",
    "See here for detailed install guide: https://github.com/abetlen/llama-cpp-python\n",
    "\n",
    "### 1.1 - Quick install for CPU \n",
    "\n",
    "\n",
    "```bash\n",
    "# adjust this to match your environment name\n",
    "conda activate atlas-1\n",
    "\n",
    "pip install llama-cpp-python\n",
    "```\n",
    "\n",
    "### 1.2 - Quick install for GPU\n",
    "\n",
    "For this you would need a Nvidia GPU with all the appropriate CUDA drivers installed.  How to install this varies from OS to OS.  ANd its beyond the scope of this guide.  So please make sure this is done, before you attempt to use GPU.\n",
    "\n",
    "```bash\n",
    "conda activate atlas-1\n",
    "\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "\n",
    "# if you need to foce reinstall\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --force-reinstall --no-cache-dir llama-cpp-python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: How do I Know if the model is using GPU\n",
    "\n",
    "To completely turn off GPU usage , set this variable before initializing the model\n",
    "\n",
    "```python\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "```\n",
    "\n",
    "See below for checks we do for GPUs.\n",
    "\n",
    "Keep in mind, even if code below says GPU is being used, unless llama-cpp-python is built with GPU support, the model won't use GPU.\n",
    "\n",
    "So double check and verify!\n",
    "\n",
    "### 2.1 - Check the logs when the model initialized\n",
    "\n",
    "You can check the output when model is initialized as follows.  IF you see something like this, that means the model is using GPU!\n",
    "\n",
    "```text\n",
    "ggml_init_cublas: found 1 CUDA devices:\n",
    "  Device 0: NVIDIA GeForce RTX 2070, compute capability 7.5, VMM: yes\n",
    "```\n",
    "\n",
    "### 2.2 - Use `nvidia-smi` tool\n",
    "\n",
    "Use `nvidia-smi` tool (run this from terminal) to see if GPU memory is being used\n",
    "\n",
    "Here is a sample output\n",
    "\n",
    "![missing image](../images/nvidia-smi-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUDA/GPU:  True\n",
      "device  0 NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "## Check if GPU is enabled\n",
    "import os\n",
    "import torch\n",
    "\n",
    "## To disable GPU and experiment, uncomment the following line\n",
    "## Normally, you would want to use GPU, if one is available.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "print (\"using CUDA/GPU: \", torch.cuda.is_available())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(\"device \", i , torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Choosing a Model\n",
    "\n",
    "Running full sized models will require a lot of resources.  So we will be using **quantized** models.  These are 'slimmed down' models that are smaller, sacrificing some quality of output\n",
    "\n",
    "See different options here : https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
    "\n",
    "The smallest recommended model is 4 bit quantized: **Q4_K_M**  - it's a medium model that provides balanced quality.\n",
    "\n",
    "So the filename is : **mistral-7b-instruct-v0.2.Q4_K_M.gguf**\n",
    "\n",
    "Go ahead and download the model from here.\n",
    "\n",
    "Here is a direct link to : [mistral-7b-instruct-v0.2.Q4_K_M.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/raw/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf)\n",
    "\n",
    "![missing image](../images/mistral-model-selection-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_file_path = '../models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf'\n",
    "# model_file_path = '../models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q5_K_M.gguf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Initialize the Model\n",
    "\n",
    "Here we are going to init the model, with the following params:\n",
    "\n",
    "- **model_path** : point to where the model file was downloaded\n",
    "- **n_gpu_layers**: controls if model is off loaded into GPU\n",
    "    - set to at least 1 to use GPU\n",
    "    - change this value from 1, 10, 20, 30, 40\n",
    "    - set to -1 to offload all layers to GPU\n",
    "    - for Nvidia GEForce 2070 with 8 GB RAM 40 works well \n",
    "    - the `mistral-7b-instruct-v0.2.Q4_K_M.gguf` consumes about 5 GB of GPU RAM with full offload (-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ../models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2070, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =    73.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "      model_path=model_file_path,\n",
    "      n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "    #   n_ctx=2048, # Uncomment to increase the context window\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: Let's test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     119.85 ms\n",
      "llama_print_timings:      sample time =      10.14 ms /    32 runs   (    0.32 ms per token,  3155.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     587.17 ms /    32 runs   (   18.35 ms per token,    54.50 tokens per second)\n",
      "llama_print_timings:       total time =     653.40 ms /    33 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      " Paris, officially known as the City of Paris, is the capital city of France. It is a major European political and cultural centre, and one of the world\n",
      "-----\n",
      "{   'choices': [   {   'finish_reason': 'length',\n",
      "                       'index': 0,\n",
      "                       'logprobs': None,\n",
      "                       'text': ' Paris, officially known as the City of Paris, '\n",
      "                               'is the capital city of France. It is a major '\n",
      "                               'European political and cultural centre, and '\n",
      "                               'one of the world'}],\n",
      "    'created': 1711177782,\n",
      "    'id': 'cmpl-fab4bce3-927b-479e-9877-7d73dc262f3b',\n",
      "    'model': '../models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
      "    'object': 'text_completion',\n",
      "    'usage': {'completion_tokens': 32, 'prompt_tokens': 13, 'total_tokens': 45}}\n",
      "CPU times: user 641 ms, sys: 15.3 ms, total: 656 ms\n",
      "Wall time: 657 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pprint\n",
    "\n",
    "output = llm(\n",
    "   \"Q: What is the capital of France? A: \", # Prompt\n",
    "   max_tokens=32,  # Generate up to this many tokens, set to None to generate up to the end of the context window\n",
    "   stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "   echo=False  # Echo the prompt back in the output\n",
    ")\n",
    "## Note  how the answer is truncated, after reaching 'max_tokens' ?\n",
    "## Set the max_tokens to None and see what you get\n",
    "print ('-----', flush=True)\n",
    "print (output['choices'][0]['text'])\n",
    "print ('-----', flush=True)\n",
    "pprint.pprint (output, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     119.85 ms\n",
      "llama_print_timings:      sample time =      53.44 ms /   154 runs   (    0.35 ms per token,  2881.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =      58.92 ms /     8 tokens (    7.37 ms per token,   135.78 tokens per second)\n",
      "llama_print_timings:        eval time =    2617.43 ms /   153 runs   (   17.11 ms per token,    58.45 tokens per second)\n",
      "llama_print_timings:       total time =    3012.81 ms /   161 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      " I'll give you a hint: There are eight of them, and they all revolve around the Sun.\n",
      "\n",
      "Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune. Each planet has unique characteristics that set it apart from the others. Mercury is the smallest and closest to the sun, while Neptune is the farthest and largest. Earth is the only known planet to harbor life. Jupiter is the largest planet, and Saturn is known for its prominent ring system. Uranus is tilted on its axis, while Mars is called the Red Planet due to its reddish appearance. All eight planets provide fascinating insights into the wonders of our universe.\n",
      "-----\n",
      "{   'choices': [   {   'finish_reason': 'stop',\n",
      "                       'index': 0,\n",
      "                       'logprobs': None,\n",
      "                       'text': \" I'll give you a hint: There are eight of \"\n",
      "                               'them, and they all revolve around the Sun.\\n'\n",
      "                               '\\n'\n",
      "                               'Mercury, Venus, Earth, Mars, Jupiter, Saturn, '\n",
      "                               'Uranus, Neptune. Each planet has unique '\n",
      "                               'characteristics that set it apart from the '\n",
      "                               'others. Mercury is the smallest and closest to '\n",
      "                               'the sun, while Neptune is the farthest and '\n",
      "                               'largest. Earth is the only known planet to '\n",
      "                               'harbor life. Jupiter is the largest planet, '\n",
      "                               'and Saturn is known for its prominent ring '\n",
      "                               'system. Uranus is tilted on its axis, while '\n",
      "                               'Mars is called the Red Planet due to its '\n",
      "                               'reddish appearance. All eight planets provide '\n",
      "                               'fascinating insights into the wonders of our '\n",
      "                               'universe.'}],\n",
      "    'created': 1711177706,\n",
      "    'id': 'cmpl-0e3d440a-2bc4-450d-9ebf-2f1cc0609de1',\n",
      "    'model': '../models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
      "    'object': 'text_completion',\n",
      "    'usage': {   'completion_tokens': 153,\n",
      "                 'prompt_tokens': 9,\n",
      "                 'total_tokens': 162}}\n",
      "CPU times: user 3 s, sys: 15.2 ms, total: 3.02 s\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pprint\n",
    "\n",
    "output = llm(\n",
    "   # \"Q: Name the planets in the Solar system.  A: \",\n",
    "   \"Name the planets in the Solar system.\",\n",
    "   max_tokens=None,\n",
    "   #stop=[\"Q:\"],\n",
    "   echo=False\n",
    ")\n",
    "\n",
    "print ('-----', flush=True)\n",
    "print (output['choices'][0]['text'])\n",
    "print ('-----', flush=True)\n",
    "pprint.pprint (output, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     119.85 ms\n",
      "llama_print_timings:      sample time =      10.05 ms /    30 runs   (    0.34 ms per token,  2985.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =      48.55 ms /     7 tokens (    6.94 ms per token,   144.20 tokens per second)\n",
      "llama_print_timings:        eval time =     499.06 ms /    29 runs   (   17.21 ms per token,    58.11 tokens per second)\n",
      "llama_print_timings:       total time =     612.97 ms /    36 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "\n",
      "\n",
      "Soft, stealthy paws tread,\n",
      "Silent hunters in night,\n",
      "Curious, playful feline.\n",
      "-----\n",
      "{   'choices': [   {   'finish_reason': 'stop',\n",
      "                       'index': 0,\n",
      "                       'logprobs': None,\n",
      "                       'text': '\\n'\n",
      "                               '\\n'\n",
      "                               'Soft, stealthy paws tread,\\n'\n",
      "                               'Silent hunters in night,\\n'\n",
      "                               'Curious, playful feline.'}],\n",
      "    'created': 1711177709,\n",
      "    'id': 'cmpl-499f7986-21d3-4a63-ae32-30890950a00c',\n",
      "    'model': '../models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
      "    'object': 'text_completion',\n",
      "    'usage': {'completion_tokens': 29, 'prompt_tokens': 8, 'total_tokens': 37}}\n",
      "CPU times: user 616 ms, sys: 2.81 ms, total: 619 ms\n",
      "Wall time: 617 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pprint\n",
    "\n",
    "output = llm(\n",
    "   #\"Q: Write a Haiku about cats.  A: \",\n",
    "   \"Write a Haiku about cats.\",\n",
    "   max_tokens=None,\n",
    "   #stop=[\"Q:\", \"\\n\"],\n",
    "   echo=False\n",
    ")\n",
    "\n",
    "print ('-----', flush=True)\n",
    "print (output['choices'][0]['text'])\n",
    "print ('-----', flush=True)\n",
    "pprint.pprint (output, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
