{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a Local Model\n",
    "\n",
    "This notebook shows how to run a local model on your computer\n",
    "\n",
    "We will be using llama CPP\n",
    "\n",
    "References:\n",
    "\n",
    "- From : https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html\n",
    "- https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/llm/llama_2_llama_cpp.ipynb\n",
    "- https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp.html\n",
    "- https://colab.research.google.com/drive/1UoPcoiA5EOBghxWKWduQhChliMHxla7U?usp=sharing#scrollTo=V1DwVkHCNDgT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Installing LLama-CPP-Python\n",
    "\n",
    "See here for detailed install guide: https://github.com/abetlen/llama-cpp-python\n",
    "\n",
    "### Quick install for CPU \n",
    "\n",
    "\n",
    "```bash\n",
    "# adjust this to match your environment name\n",
    "conda activate atlas-1\n",
    "\n",
    "pip install llama-cpp-python\n",
    "```\n",
    "\n",
    "### Quick install for GPU\n",
    "\n",
    "For this you would need a Nvidia GPU with all the appropriate CUDA drivers installed.  How to install this varies from OS to OS.  ANd its beyond the scope of this guide.  So please make sure this is done, before you attempt to use GPU.\n",
    "\n",
    "```bash\n",
    "conda activate atlas-1\n",
    "\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "\n",
    "# if you need to foce reinstall\n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --force-reinstall --no-cache-dir llama-cpp-python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: How do I Know if the model is using GPU\n",
    "\n",
    "To completely turn off GPU usage , set this variable before initializing the model\n",
    "\n",
    "```python\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "```\n",
    "\n",
    "See below for checks we do for GPUs.\n",
    "\n",
    "Keep in mind, even if code below says GPU is being used, unless llama-cpp-python is built with GPU support, the model won't use GPU.\n",
    "\n",
    "So double check and verify!\n",
    "\n",
    "### 2.1 - Check the logs when the model initialized\n",
    "\n",
    "You can check the output when model is initialized as follows.  IF you see something like this, that means the model is using GPU!\n",
    "\n",
    "```text\n",
    "ggml_init_cublas: found 1 CUDA devices:\n",
    "  Device 0: NVIDIA GeForce RTX 2070, compute capability 7.5, VMM: yes\n",
    "```\n",
    "\n",
    "### 2.2 - Use `nvidia-smi` tool\n",
    "\n",
    "Use `nvidia-smi` tool (run this from terminal) to see if GPU memory is being used\n",
    "\n",
    "Here is a sample output\n",
    "\n",
    "![missing image](../images/nvidia-smi-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUDA/GPU:  True\n",
      "device  0 NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "## Check if GPU is enabled\n",
    "import os\n",
    "import torch\n",
    "\n",
    "## To disable GPU and experiment, uncomment the following line\n",
    "## Normally, you would want to use GPU, if one is available.\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "print (\"using CUDA/GPU: \", torch.cuda.is_available())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(\"device \", i , torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Choosing a Model\n",
    "\n",
    "Running full sized models will require a lot of resources.  So we will be using **quantized** models.  These are 'slimmed down' models that are smaller, sacrificing some quality of output\n",
    "\n",
    "See different options here : https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
    "\n",
    "The smallest recommended model is 4 bit quantized: **Q4_K_M**  - it's a medium model that provides balanced quality.\n",
    "\n",
    "So the filename is : **mistral-7b-instruct-v0.2.Q4_K_M.gguf**\n",
    "\n",
    "Go ahead and download the model from here.\n",
    "\n",
    "Here is a direct link to : [mistral-7b-instruct-v0.2.Q4_K_M.gguf](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/raw/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf)\n",
    "\n",
    "![missing image](../images/mistral-model-selection-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = '../models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf'\n",
    "# model_file_path = '../models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q5_K_M.gguf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Initialize the Model\n",
    "\n",
    "Here we are going to init the model, with the following params:\n",
    "\n",
    "- **model_path** : point to where the model file was downloaded\n",
    "- **n_gpu_layers**: controls if model is off loaded into GPU\n",
    "    - set to at least 1 to use GPU\n",
    "    - change this value from 1, 10, 20, 30, 40\n",
    "    - set to -1 to offload all layers to GPU\n",
    "    - for Nvidia GEForce 2070 with 8 GB RAM 40 works well \n",
    "    - the `mistral-7b-instruct-v0.2.Q4_K_M.gguf` consumes about 5 GB of GPU RAM with full offload (-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/anaconda3/envs/atlas2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ../models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2070, compute capability 7.5, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  4095.05 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   487.50 MiB\n",
      "llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   283.37 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    15.62 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=model_file_path,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU,  0 for no GPU, -1 to offload every thing to GPU\n",
    "    # change this value from 1, 10, 20, 30, 40\n",
    "    # for Nvidia GEForce 2070 with 8 GB RAM 40 works well\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: Let's test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     135.94 ms\n",
      "llama_print_timings:      sample time =       3.11 ms /     9 runs   (    0.35 ms per token,  2893.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     135.73 ms /    69 tokens (    1.97 ms per token,   508.35 tokens per second)\n",
      "llama_print_timings:        eval time =     128.94 ms /     8 runs   (   16.12 ms per token,    62.04 tokens per second)\n",
      "llama_print_timings:       total time =     287.69 ms /    77 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      " The capital city of France is Paris.\n",
      "-----\n",
      "{   'additional_kwargs': {},\n",
      "    'delta': None,\n",
      "    'logprobs': None,\n",
      "    'raw': {   'choices': [   {   'finish_reason': 'stop',\n",
      "                                  'index': 0,\n",
      "                                  'logprobs': None,\n",
      "                                  'text': ' The capital city of France is '\n",
      "                                          'Paris.'}],\n",
      "               'created': 1711177447,\n",
      "               'id': 'cmpl-a853953a-739b-4320-93a0-cfcfdb9a1474',\n",
      "               'model': '../models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
      "               'object': 'text_completion',\n",
      "               'usage': {   'completion_tokens': 8,\n",
      "                            'prompt_tokens': 69,\n",
      "                            'total_tokens': 77}},\n",
      "    'text': ' The capital city of France is Paris.'}\n",
      "CPU times: user 253 ms, sys: 40.7 ms, total: 294 ms\n",
      "Wall time: 292 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pprint\n",
    "\n",
    "prompt = \"What is the capital of France\"\n",
    "\n",
    "response = llm.complete(prompt)\n",
    "print ('-----', flush=True)\n",
    "print(response.text)\n",
    "print ('-----', flush=True)\n",
    "pprint.pprint (response.__dict__, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     135.94 ms\n",
      "llama_print_timings:      sample time =       8.69 ms /    26 runs   (    0.33 ms per token,  2991.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =      84.93 ms /    10 tokens (    8.49 ms per token,   117.75 tokens per second)\n",
      "llama_print_timings:        eval time =     399.03 ms /    25 runs   (   15.96 ms per token,    62.65 tokens per second)\n",
      "llama_print_timings:       total time =     533.98 ms /    35 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      " Silent paws glide,\n",
      "Soft purrs soothe troubled hearts,\n",
      "Grace in feline form.\n",
      "-----\n",
      "{   'additional_kwargs': {},\n",
      "    'delta': None,\n",
      "    'logprobs': None,\n",
      "    'raw': {   'choices': [   {   'finish_reason': 'stop',\n",
      "                                  'index': 0,\n",
      "                                  'logprobs': None,\n",
      "                                  'text': ' Silent paws glide,\\n'\n",
      "                                          'Soft purrs soothe troubled hearts,\\n'\n",
      "                                          'Grace in feline form.'}],\n",
      "               'created': 1711177447,\n",
      "               'id': 'cmpl-a26f73cc-a2fc-4ff2-98dd-b6d3157d3d5b',\n",
      "               'model': '../models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
      "               'object': 'text_completion',\n",
      "               'usage': {   'completion_tokens': 25,\n",
      "                            'prompt_tokens': 69,\n",
      "                            'total_tokens': 94}},\n",
      "    'text': ' Silent paws glide,\\n'\n",
      "            'Soft purrs soothe troubled hearts,\\n'\n",
      "            'Grace in feline form.'}\n",
      "CPU times: user 534 ms, sys: 5.14 ms, total: 539 ms\n",
      "Wall time: 538 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# prompt = \"Hello! Can you tell me a poem about cats and dogs?\"\n",
    "prompt = \"Write a haiku about cats\"\n",
    "\n",
    "response = llm.complete(prompt)\n",
    "print ('-----', flush=True)\n",
    "print(response.text)\n",
    "print ('-----', flush=True)\n",
    "pprint.pprint (response.__dict__, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     135.94 ms\n",
      "llama_print_timings:      sample time =      45.84 ms /   133 runs   (    0.34 ms per token,  2901.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =      86.62 ms /    10 tokens (    8.66 ms per token,   115.45 tokens per second)\n",
      "llama_print_timings:        eval time =    2125.97 ms /   132 runs   (   16.11 ms per token,    62.09 tokens per second)\n",
      "llama_print_timings:       total time =    2539.75 ms /   142 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      " In our Solar System, there are eight planets. In order from the Sun: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Each of these planets has unique characteristics that distinguish it from the others. Mercury is the smallest and closest to the Sun, while Jupiter is the largest and known for its Great Red Spot. Saturn is famous for its prominent ring system, while Uranus is unique for being tilted on its side. Neptune is known for its deep blue color and violent weather patterns. I'm here to help with any questions you might have!\n",
      "-----\n",
      "{   'additional_kwargs': {},\n",
      "    'delta': None,\n",
      "    'logprobs': None,\n",
      "    'raw': {   'choices': [   {   'finish_reason': 'stop',\n",
      "                                  'index': 0,\n",
      "                                  'logprobs': None,\n",
      "                                  'text': ' In our Solar System, there are '\n",
      "                                          'eight planets. In order from the '\n",
      "                                          'Sun: Mercury, Venus, Earth, Mars, '\n",
      "                                          'Jupiter, Saturn, Uranus, and '\n",
      "                                          'Neptune. Each of these planets has '\n",
      "                                          'unique characteristics that '\n",
      "                                          'distinguish it from the others. '\n",
      "                                          'Mercury is the smallest and closest '\n",
      "                                          'to the Sun, while Jupiter is the '\n",
      "                                          'largest and known for its Great Red '\n",
      "                                          'Spot. Saturn is famous for its '\n",
      "                                          'prominent ring system, while Uranus '\n",
      "                                          'is unique for being tilted on its '\n",
      "                                          'side. Neptune is known for its deep '\n",
      "                                          'blue color and violent weather '\n",
      "                                          \"patterns. I'm here to help with any \"\n",
      "                                          'questions you might have!'}],\n",
      "               'created': 1711177448,\n",
      "               'id': 'cmpl-ee3f1a33-1747-4ca3-ab93-067a59aa5b56',\n",
      "               'model': '../models/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/mistral-7b-instruct-v0.2.Q4_K_M.gguf',\n",
      "               'object': 'text_completion',\n",
      "               'usage': {   'completion_tokens': 132,\n",
      "                            'prompt_tokens': 69,\n",
      "                            'total_tokens': 201}},\n",
      "    'text': ' In our Solar System, there are eight planets. In order from the '\n",
      "            'Sun: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and '\n",
      "            'Neptune. Each of these planets has unique characteristics that '\n",
      "            'distinguish it from the others. Mercury is the smallest and '\n",
      "            'closest to the Sun, while Jupiter is the largest and known for '\n",
      "            'its Great Red Spot. Saturn is famous for its prominent ring '\n",
      "            'system, while Uranus is unique for being tilted on its side. '\n",
      "            'Neptune is known for its deep blue color and violent weather '\n",
      "            \"patterns. I'm here to help with any questions you might have!\"}\n",
      "CPU times: user 2.49 s, sys: 60.2 ms, total: 2.55 s\n",
      "Wall time: 2.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# prompt = \"Q: Name the planets in Solar system.  A:\"\n",
    "prompt = \"Name the planets in Solar system\"\n",
    "\n",
    "response = llm.complete(prompt)\n",
    "print ('-----', flush=True)\n",
    "print(response.text)\n",
    "print ('-----', flush=True)\n",
    "pprint.pprint (response.__dict__, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
