{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-taohUytQcEh"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sujee/mongodb-atlas-vector-search/blob/main/lab-4-rag/rag-10k-a-populate-embeddings-mistral.ipynb)\n",
        "\n",
        "#  RAG-10k - Populate data with Mistral embeddings\n",
        "\n",
        "## Overview\n",
        "\n",
        "Here is overall RAG pipeline.  In this notebook we will do step-1.\n",
        "\n",
        "This notebook showcases how to use **MISTRAL EMBEDDING MODEL** to create embeddings.\n",
        "\n",
        "We will do the following:\n",
        "\n",
        "- ğŸ‘‰ Load PDF documents\n",
        "- ğŸ‘‰ Use Mistral embedding models to calculate embeddings for PDF documents\n",
        "- ğŸ‘‰ Upload them into Atlas\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/sujee/mongodb-atlas-vector-search/main/images/rag-1.svg)\n",
        "\n",
        "### What you need to run this notebook\n",
        "\n",
        "- a (free) MongoDB Atlas Account\n",
        "- and connection credentials\n",
        "- a Mistral API Key\n",
        "\n",
        "### The Stack\n",
        "\n",
        "- Langugage : Python\n",
        "- Vector database: Atlas\n",
        "- Embedding Model: Mistral embedding model\n",
        "\n",
        "\n",
        "### How to run\n",
        "\n",
        "This notebook can be run on Google Colab and stand alone python development environments.  Click here to run on colab.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sujee/mongodb-atlas-vector-search/blob/main/lab-4-rag/rag-10k-a-populate-embeddings-mistral.ipynb)\n",
        "\n",
        "\n",
        "References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2SmttnIQcEi"
      },
      "source": [
        "## Step-1: Setup Atlas\n",
        "\n",
        "We will need to have Atlas setup.\n",
        "\n",
        "Follow [instructions here](https://github.com/sujee/mongodb-atlas-vector-search/blob/main/lab-1-atlas-setup/setup-atlas.md)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTx5QYBSTjX9"
      },
      "source": [
        "## Step-2: Configuration\n",
        "\n",
        "We will setup some common configurations here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w8Aqlv4mTo_R"
      },
      "outputs": [],
      "source": [
        "# We will keep all global variables in an object to not pollute the global namespace.\n",
        "class MyConfig(object):\n",
        "    pass\n",
        "\n",
        "MY_CONFIG = MyConfig()\n",
        "\n",
        "MY_CONFIG.DB_NAME = 'rag1'\n",
        "MY_CONFIG.COLLECTION_NAME = '10k_mistral'\n",
        "MY_CONFIG.EMBEDDING_ATTRIBUTE = 'embedding_mistral'\n",
        "MY_CONFIG.INDEX_NAME = 'idx_embedding_mistral'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE3eUc9lQcEi"
      },
      "source": [
        "## Step-3: Load Configuration\n",
        "\n",
        "We need to configure the following\n",
        "- Atlas connection credentials\n",
        "\n",
        "### Option 3A - If running on Colab\n",
        "\n",
        "- Click on 'Colab secrets' icon (ğŸ”‘) on left pane, and crate the following secrets.\n",
        "   - `ATLAS_URI`\n",
        "   - `MISTRAL_API_KEY`\n",
        "-  Make sure the `notebook access` button is checked on for all\n",
        "- See screenshot below for example\n",
        "\n",
        "<!-- ![](../images/colab-secret-2.png) -->\n",
        "\n",
        "![](https://raw.githubusercontent.com/sujee/mongodb-atlas-vector-search/main/images/colab-secret-3.png)\n",
        "\n",
        "\n",
        "### Option 3B - If running on local python environment\n",
        "\n",
        "- setup your local python env following this [setup guide](https://github.com/sujee/mongodb-atlas-vector-search/blob/main/setup-python-env.md)\n",
        "- Create a file named `.env` in the same location as notebook\n",
        "- And add the following settings\n",
        "\n",
        "```text\n",
        "ATLAS_URI=mongodb+srv://<username>:<password>@sandbox.....\n",
        "MISTRAL_API_KEY=xyz\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSrnmpPLQcEi"
      },
      "source": [
        "## Step-4: Determine Runtime Environment\n",
        "\n",
        "This code will figure out if we are running on Google Colab environment or local environment.  We use it to install relevant packages later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHQlvRw4QcEj",
        "outputId": "ed3d6100-0f2f-492c-8a42-a4ec94997124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in Colab\n"
          ]
        }
      ],
      "source": [
        "# are we running in Colab?\n",
        "import os\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "    print(\"Running in Colab\")\n",
        "    MY_CONFIG.RUNNING_IN_COLAB = True\n",
        "else:\n",
        "    print(\"NOT running in Colab\")\n",
        "    MY_CONFIG.RUNNING_IN_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzwRJSd1QcEk"
      },
      "source": [
        "## Step-5: Install dependencies (if necessary)\n",
        "\n",
        "We will install required libraries in cloud environments like Google Colab.  For local environments, we assume the dependencies are already setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3BCCw3wQcEk",
        "outputId": "f3b55bdb-bf30-41fe-8b4a-1a3d265602fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymongo==4.6.2\n",
            "  Downloading pymongo-4.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m677.2/677.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index\n",
            "  Downloading llama_index-0.10.20-py3-none-any.whl (5.6 kB)\n",
            "Collecting llama-index-embeddings-huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.1.4-py3-none-any.whl (7.7 kB)\n",
            "Collecting llama-index-embeddings-mistralai\n",
            "  Downloading llama_index_embeddings_mistralai-0.1.4-py3-none-any.whl (2.6 kB)\n",
            "Collecting llama-index-vector-stores-mongodb\n",
            "  Downloading llama_index_vector_stores_mongodb-0.1.4-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: transformers==4.38.2 in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo==4.6.2)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Collecting llama-index-agent-openai<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.1.6-py3-none-any.whl (12 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_cli-0.1.10-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core<0.11.0,>=0.10.20 (from llama-index)\n",
            "  Downloading llama_index_core-0.10.20.post2-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.7-py3-none-any.whl (6.0 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.1.4-py3-none-any.whl (6.6 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.1.12-py3-none-any.whl (10 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.4-py3-none-any.whl (5.8 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.1.11-py3-none-any.whl (36 kB)\n",
            "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.3-py3-none-any.whl (2.5 kB)\n",
            "Collecting mistralai>=0.1.3 (from llama-index-embeddings-mistralai)\n",
            "  Downloading mistralai-0.1.6-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (3.9.3)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (2.0.28)\n",
            "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Collecting llamaindex-py-client<0.2.0,>=0.1.13 (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
            "  Downloading llamaindex_py_client-0.1.13-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (1.6.0)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (1.5.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (9.4.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.20->llama-index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Collecting bs4<0.0.3,>=0.0.2 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Collecting pymupdf<2.0.0,>=1.23.21 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading PyMuPDF-1.23.26-cp310-none-manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse<0.4.0,>=0.3.3 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n",
            "  Downloading llama_parse-0.3.9-py3-none-any.whl (6.8 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.10 (from mistralai>=0.1.3->llama-index-embeddings-mistralai)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas (from llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
            "  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow<16.0.0,>=15.0.0 (from mistralai>=0.1.3->llama-index-embeddings-mistralai)\n",
            "  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.2) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.20->llama-index) (1.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.20->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.20->llama-index) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.20->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.20->llama-index) (2023.4)\n",
            "Collecting tzdata>=2022.7 (from pandas->llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
            "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.22 (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading PyMuPDFb-1.23.22-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.20->llama-index) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.20->llama-index)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.20->llama-index) (1.16.0)\n",
            "Installing collected packages: striprtf, dirtyjson, tzdata, pypdf, PyMuPDFb, pyarrow, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, h11, dnspython, deprecated, typing-inspect, tiktoken, pymupdf, pymongo, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, bs4, nvidia-cusolver-cu12, httpx, dataclasses-json, openai, mistralai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-vector-stores-mongodb, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-embeddings-mistralai, llama-index-embeddings-huggingface, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.25.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMuPDFb-1.23.22 bs4-0.0.2 dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 dnspython-2.6.1 h11-0.14.0 httpcore-1.0.4 httpx-0.25.2 llama-index-0.10.20 llama-index-agent-openai-0.1.6 llama-index-cli-0.1.10 llama-index-core-0.10.20.post2 llama-index-embeddings-huggingface-0.1.4 llama-index-embeddings-mistralai-0.1.4 llama-index-embeddings-openai-0.1.7 llama-index-indices-managed-llama-cloud-0.1.4 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.12 llama-index-multi-modal-llms-openai-0.1.4 llama-index-program-openai-0.1.4 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.11 llama-index-readers-llama-parse-0.1.3 llama-index-vector-stores-mongodb-0.1.4 llama-parse-0.3.9 llamaindex-py-client-0.1.13 marshmallow-3.21.1 mistralai-0.1.6 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 openai-1.14.2 orjson-3.9.15 pandas-2.2.1 pyarrow-15.0.2 pymongo-4.6.2 pymupdf-1.23.26 pypdf-4.1.0 striprtf-0.0.26 tiktoken-0.6.0 typing-inspect-0.9.0 tzdata-2024.1\n"
          ]
        }
      ],
      "source": [
        "if MY_CONFIG.RUNNING_IN_COLAB:\n",
        "    !pip install \\\n",
        "                pymongo==4.6.2 \\\n",
        "                llama-index \\\n",
        "                llama-index-embeddings-mistralai \\\n",
        "                llama-index-vector-stores-mongodb \\\n",
        "                transformers==4.38.2 \\\n",
        "                torch==2.2.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHhS7M_YQcEk"
      },
      "source": [
        "## Step-6: Basic Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pszq4paVQcEk"
      },
      "source": [
        "### 6.1 - Check if we have GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eiWzAqkQcEl",
        "outputId": "f91b05b2-e0d3-4b63-8fe6-a196ee2c57d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using CUDA/GPU:  False\n"
          ]
        }
      ],
      "source": [
        "## Check if GPU is enabled\n",
        "import os\n",
        "import torch\n",
        "\n",
        "## To disable GPU and experiment, uncomment the following line\n",
        "## Normally, you would want to use GPU, if one is available.\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
        "\n",
        "print (\"using CUDA/GPU: \", torch.cuda.is_available())\n",
        "\n",
        "for i in range(torch.cuda.device_count()):\n",
        "   print(\"device \", i , torch.cuda.get_device_properties(i).name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S1vk-DxQcEm"
      },
      "source": [
        "### 6.2 - Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cGmiPAQmQcEm"
      },
      "outputs": [],
      "source": [
        "## Setup logging.  To see more loging set the level to DEBUG\n",
        "\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-IBxhPUQcEm"
      },
      "source": [
        "## Step-7: Load Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwaqeg04RjQ3",
        "outputId": "d66ae258-484a-4106-a922-66be78140a46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… config ATLAS_URI found\n",
            "âœ… config MISTRAL_API_KEY found\n"
          ]
        }
      ],
      "source": [
        "## Load settings based on where we are running\n",
        "##  - if runninning on google Colab, load from secrets\n",
        "##  - if running locally use dotenv\n",
        "\n",
        "if MY_CONFIG.RUNNING_IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    MY_CONFIG.ATLAS_URI = userdata.get('ATLAS_URI')\n",
        "    MY_CONFIG.MISTRAL_API_KEY = userdata.get('MISTRAL_API_KEY')\n",
        "    # MY_CONFIG.OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "else:\n",
        "    import os, sys\n",
        "    from dotenv import find_dotenv, dotenv_values\n",
        "\n",
        "    this_dir = os.path.abspath('')\n",
        "    parent_dir = os.path.dirname(this_dir)\n",
        "    sys.path.append (os.path.abspath (parent_dir))\n",
        "\n",
        "    config = dotenv_values(find_dotenv())\n",
        "    # debug\n",
        "    # print (config)\n",
        "    MY_CONFIG.ATLAS_URI = config.get('ATLAS_URI')\n",
        "    MY_CONFIG.MISTRAL_API_KEY = config.get(\"MISTRAL_API_KEY\")\n",
        "## --- end load config\n",
        "\n",
        "## If you just want to quickly set the config manually, you can do so here.\n",
        "# MY_CONFIG.ATLAS_URI = ''\n",
        "# MY_CONFIG.MISTRAL_API_KEY = ''\n",
        "\n",
        "if  MY_CONFIG.ATLAS_URI:\n",
        "    print (\"âœ… config ATLAS_URI found\")\n",
        "else:\n",
        "    raise Exception (\"'âŒ ATLAS_URI' is not set.  Please set it above to continue...\")\n",
        "\n",
        "\n",
        "if MY_CONFIG.MISTRAL_API_KEY:\n",
        "   print (\"âœ… config MISTRAL_API_KEY found\")\n",
        "else:\n",
        "    raise Exception (\"âŒ'MISTRAL_API_KEY' is not set.  Please set it above to continue...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy1S5E_XSO5r"
      },
      "source": [
        "## Step-8 : Get Data Files (if needed)\n",
        "\n",
        "We are going to be using 10K filings - these are financial documents filed by US public companies to SEC (Securities and Exchange Commission).  You can read about them [here](https://www.investor.gov/introduction-investing/investing-basics/glossary/form-10-k)\n",
        "\n",
        "We have two 10k documents from Lyft and Uber\n",
        "\n",
        "Don't think it is just 2 documents.  Each PDF documnet is about 200+ pages long.  So these are serious PDF documents.\n",
        "\n",
        "Let's get these data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqUQn_K6VIhf",
        "outputId": "a1d67a80-49de-4cde-e9f1-04b22c15a86e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-03-20 07:16:56--  https://raw.githubusercontent.com/sujee/mongodb-atlas-vector-search/main/data/10k/lyft_2021.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1440303 (1.4M) [application/octet-stream]\n",
            "Saving to: â€˜data/10k/lyft_2021.pdfâ€™\n",
            "\n",
            "\rdata/10k/lyft_2021.   0%[                    ]       0  --.-KB/s               \rdata/10k/lyft_2021. 100%[===================>]   1.37M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-03-20 07:16:56 (44.2 MB/s) - â€˜data/10k/lyft_2021.pdfâ€™ saved [1440303/1440303]\n",
            "\n",
            "âœ… Downloaded data file : data/10k/lyft_2021.pdf\n",
            "--2024-03-20 07:16:56--  https://raw.githubusercontent.com/sujee/mongodb-atlas-vector-search/main/data/10k/uber_2021.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1880483 (1.8M) [application/octet-stream]\n",
            "Saving to: â€˜data/10k/uber_2021.pdfâ€™\n",
            "\n",
            "data/10k/uber_2021. 100%[===================>]   1.79M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-03-20 07:16:56 (42.7 MB/s) - â€˜data/10k/uber_2021.pdfâ€™ saved [1880483/1880483]\n",
            "\n",
            "âœ… Downloaded data file : data/10k/uber_2021.pdf\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# ------- begin -------\n",
        "def download_data_file (remote_file, local_file):\n",
        "     if  os.path.exists (local_file):\n",
        "         print (f\"âœ… Local data files exists : {local_file}\")\n",
        "     else:\n",
        "        !wget -O {local_file}  {remote_file}\n",
        "        print (f\"âœ… Downloaded data file : {local_file}\")\n",
        "#-------- end -------\n",
        "\n",
        "# figure out data dir\n",
        "if MY_CONFIG.RUNNING_IN_COLAB:\n",
        "    MY_CONFIG.DATA_DIR = \"data/10k\"\n",
        "else:\n",
        "    MY_CONFIG.DATA_DIR = \"../data/10k\"\n",
        "\n",
        "if not os.path.exists (MY_CONFIG.DATA_DIR):\n",
        "  !mkdir -p {MY_CONFIG.DATA_DIR}\n",
        "\n",
        "download_data_file ('https://raw.githubusercontent.com/sujee/mongodb-atlas-vector-search/main/data/10k/lyft_2021.pdf',\n",
        "                    os.path.join (MY_CONFIG.DATA_DIR, 'lyft_2021.pdf'))\n",
        "\n",
        "download_data_file ('https://raw.githubusercontent.com/sujee/mongodb-atlas-vector-search/main/data/10k/uber_2021.pdf',\n",
        "                    os.path.join (MY_CONFIG.DATA_DIR, 'uber_2021.pdf'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJtrApMjQcEm"
      },
      "source": [
        "## Step-9: Inspect the PDF files\n",
        "\n",
        "The will be in the following directory\n",
        "\n",
        "\n",
        "```text\n",
        "data/10k/\n",
        "â”œâ”€â”€ lyft_2021.pdf\n",
        "â””â”€â”€ uber_2021.pdf\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D7dcN6sZBO3"
      },
      "source": [
        "## Step-10: Initialize Atlas Client\n",
        "\n",
        "If this step fails, make sure 'connect from anywhere' is enabled on your Atlas network configuration\n",
        "\n",
        "![](https://raw.githubusercontent.com/sujee/mongodb-atlas-vector-search/main/images/atlas-connect-2.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oGxcy-aQcEn",
        "outputId": "5847ae37-fb9d-4329-d98f-81038ba17806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Connected to Atlas instance!\n"
          ]
        }
      ],
      "source": [
        "import pymongo\n",
        "\n",
        "mongodb_client = pymongo.MongoClient(MY_CONFIG.ATLAS_URI)\n",
        "print ('âœ… Connected to Atlas instance!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZwqfx0HQcEo"
      },
      "source": [
        "## (Optional) Step-11: Clear out the collection\n",
        "\n",
        "For a fresh start!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nxOcRrkvQcEo"
      },
      "outputs": [],
      "source": [
        "## if a clean start is required, you can use the following code to clear out old data\n",
        "\n",
        "database = mongodb_client[MY_CONFIG.DB_NAME]\n",
        "collection = database [MY_CONFIG.COLLECTION_NAME]\n",
        "\n",
        "doc_count = collection.count_documents (filter = {})\n",
        "print (f\"Document count before delete : {doc_count:,}\")\n",
        "\n",
        "result = collection.delete_many(filter= {})\n",
        "print (f\"Deleted docs : {result.deleted_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuIWbCQEQcEo"
      },
      "source": [
        "## Step-12: Calculate Embeddings\n",
        "\n",
        "There are many choices here:\n",
        "\n",
        "* OpenAI embeddings - call via API  (See sample notebook here )\n",
        "* **MistralAI embeddings - call via API  (this notebook)**\n",
        "* Local embedding models (See notebook here)\n",
        "\n",
        "We are going to use Llama-index-mistral package ([documentation](https://docs.llamaindex.ai/en/stable/examples/embeddings/mistralai.html))\n",
        "\n",
        "We will call\n",
        "\n",
        "```python\n",
        "MistralAIEmbedding(model_name=model_name, api_key=api_key)\n",
        "```\n",
        "\n",
        "Our model name would be \"mistral-embed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YzzXKRsFQcEp"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "\n",
        "Settings.embed_model = MistralAIEmbedding(model_name='mistral-embed', api_key=MY_CONFIG.MISTRAL_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZoBOqLdxetut"
      },
      "outputs": [],
      "source": [
        "## testing\n",
        "embeddings = Settings.embed_model.get_text_embedding(\"La Plateforme - The Platform\")\n",
        "print ('embedding len : ', len(embeddings))\n",
        "print ('first few embeddings : ', embeddings[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z3jQ8eTQcEp"
      },
      "source": [
        "## Step-13: Connect Illama-Index and MongoDB Atlas\n",
        "\n",
        "Let's define MongoDB Atlas as our vector storage. This is critical to stored indexed data and then query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ma94ZSbsQcEp"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "\n",
        "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
        "                                        db_name = MY_CONFIG.DB_NAME,\n",
        "                                        collection_name = MY_CONFIG.COLLECTION_NAME,\n",
        "                                        index_name  = MY_CONFIG.INDEX_NAME,\n",
        "                                        embedding_key = MY_CONFIG.EMBEDDING_ATTRIBUTE,\n",
        "                                        ## the following columns are set to default values\n",
        "                                       # text_key = 'text', metadata_= 'metadata',\n",
        "                                 )\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJX1IUmhQcEq"
      },
      "source": [
        "## Step-14: Read PDF Documents\n",
        "\n",
        "Ilmaa-index has very handy `SimpleDirectoryReader` that can read single files / multiple files / or entire directory content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5QMS8MFQcEq",
        "outputId": "b12f4ee1-fabc-4acc-d54b-57797cee9ebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 545 chunks from 'data/10k'\n",
            "CPU times: user 26.4 s, sys: 163 ms, total: 26.6 s\n",
            "Wall time: 28 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "\n",
        "## This reads one doc\n",
        "# docs = SimpleDirectoryReader(\n",
        "#     input_files=[\"./data/10k/uber_2021.pdf\"]\n",
        "# ).load_data()\n",
        "\n",
        "## here we read entire directory content\n",
        "docs = SimpleDirectoryReader(\n",
        "        input_dir=MY_CONFIG.DATA_DIR\n",
        ").load_data()\n",
        "\n",
        "print (f\"Loaded {len(docs)} chunks from '{MY_CONFIG.DATA_DIR}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHkT6HNqQcEq"
      },
      "source": [
        "## Step-15: Index the docs and Store them into MongoDB Atlas\n",
        "\n",
        "When we execute the code below, the following will happen\n",
        "\n",
        "- documents are indexed\n",
        "- embeddings are created for text\n",
        "- the document (text, embeddings) are stored in our Vector Storage (MongoDB Atlas in this case)\n",
        "\n",
        "**Note: This might take a couple of minutes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vCbgdi8QcEq",
        "outputId": "886c0886-e4c8-4251-892a-40a913eac908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 7.83 s, sys: 228 ms, total: 8.05 s\n",
            "Wall time: 53.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    docs, storage_context=storage_context\n",
        ")\n",
        "\n",
        "refreshed_index = index.refresh_ref_docs (docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCy5bD_aQcEq"
      },
      "source": [
        "## Step-16: View Created Documents in Atlas\n",
        "\n",
        "- Go to your Atlas dashboard\n",
        "- Select 'browse collections'\n",
        "- Select database: **`rag1`**  and collection **`10k_mistral`**\n",
        "- Click around to see some sample data inserted\n",
        "- You will see `text` attribute having text data\n",
        "- `embeddings` are populated too\n",
        "- expand the `meta` attribute.  This is automatically populated for us by llama-index\n",
        "\n",
        "![](https://raw.githubusercontent.com/sujee/mongodb-atlas-vector-search/main/images/10k-documents-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cllBbJvJQcEq"
      },
      "source": [
        "## Step-17: Setup Vector Index\n",
        "\n",
        "Before we do vector search, we need to define an embedding index\n",
        "\n",
        "You can look at steps here [setup-atlas-index.md](setup-atlas-index.md)\n",
        "\n",
        "Here are the details:\n",
        "\n",
        "- database : **`rag1`**\n",
        "- Collection: **`10k_mistral`**\n",
        "- index_name = **`idx_embedding_mistral`**\n",
        "\n",
        "index defitintion json\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"fields\": [\n",
        "    {\n",
        "      \"type\": \"vector\",\n",
        "      \"path\": \"embedding_mistral\",\n",
        "      \"numDimensions\": 1024,\n",
        "      \"similarity\": \"dotProduct\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "The similarity function can be  one of\n",
        "- \"euclidean\"\n",
        "- \"cosine\"\n",
        "- \"dotProduct\"\n",
        "\n",
        "\n",
        "### Follow these steps to setup index\n",
        "\n",
        "\n",
        "![missing image](https://raw.githubusercontent.com/sujee/mongodb-atlas-vector-search/main/images/atlas-index-2.png)\n",
        "\n",
        "![missing image](https://raw.githubusercontent.com/sujee/mongodb-atlas-vector-search/main/images/atlas-index-rag-mistral-1.png)\n",
        "\n",
        "![missing image](https://raw.githubusercontent.com/sujee/mongodb-atlas-vector-search/main/images/atlas-index-rag-mistral-2.png)\n",
        "\n",
        "## We are done! ğŸ‘\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa1IUy98QcEr"
      },
      "source": [
        "## We are done\n",
        "\n",
        "Now the data is populated and ready to be queried.\n",
        "\n",
        "Let's go to the next lab: query"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
